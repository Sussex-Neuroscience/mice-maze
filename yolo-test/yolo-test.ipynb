{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Object Tracking Test \n",
    "\n",
    "This notebook explores the possiblity of making use of the YOLO algorithm to perform tracking of the mouse moving within a maze. \n",
    "\n",
    "YOLO is a machine learning model designed to perform real-time object localization and detection and is widely used for a variety of computer vision based tasks. It's well known for its impressive speed and accuracy, and ease of use. \n",
    "\n",
    "In this experiment, we use YOLO v8.2 to perform image localization. This is the process of identifying a set of two points on the image, known as the bounding box, which surround an object within the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt \n",
    "import ultralytics \n",
    "import os \n",
    "from typing import Optional\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "VIDEO_PATH = \"/Users/henrywilliams/Downloads/videos/maze_test.mp4\" \n",
    "\n",
    "yolo = ultralytics.YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataset\n",
    "\n",
    "The following code cell takes 250 random frames from the video capture, and saves them to the `./frames/` directory. These frames are later annotated and used to fine-tune the YOLO model. To change where the frames are saved, modify the `OUTPUT_DIR` variable, and in order to change the number of samples captured, modify the `TOTAL_SAMPLES` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Un-annotated dataset already exists, skipping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(OUTPUT_DIR)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUn-annotated dataset already exists, skipping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(TOTAL_SAMPLES):\n\u001b[1;32m     30\u001b[0m     n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_frames)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Un-annotated dataset already exists, skipping"
     ]
    }
   ],
   "source": [
    "TOTAL_SAMPLES = 250\n",
    "OUTPUT_DIR = './frames'\n",
    "\n",
    "def get_nth_frame(cap: cv2.VideoCapture, n: int, n_frames: Optional[int] = None) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Retrieve the nth frame from a video capture object.\n",
    "\n",
    "    This function extracts the nth frame from a given video capture object `cap`. \n",
    "    If `n_frames` is not provided, the function will determine the total number of frames \n",
    "    in the video. The frame counter is reset to its last value before the nth frame was \n",
    "    retrieved\n",
    "\n",
    "    Parameters:\n",
    "    cap (cv2.VideoCapture): The video capture object from which to extract the frame.\n",
    "    n (int): The frame number to retrieve.\n",
    "    n_frames (Optional[int], optional): The total number of frames in the video. \n",
    "                                        If not provided, it will be determined automatically.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The nth frame as an array.\n",
    "\n",
    "    Raises:\n",
    "    IndexError: If `n` exceeds the total number of frames available in the video.\n",
    "    RuntimeError: If the frame could not be read from the video capture object.\n",
    "\n",
    "    Example usage:\n",
    "    >>> cap = cv2.VideoCapture('video.mp4')\n",
    "    >>> frame = get_nth_frame(cap, 10)\n",
    "    >>> cv2.imshow('10th Frame', frame)\n",
    "    >>> cv2.waitKey(0)\n",
    "    >>> cv2.destroyAllWindows()\n",
    "    \"\"\"\n",
    "    n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) if n_frames is None else n_frames\n",
    "    current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "    if n > n_frames: \n",
    "        raise IndexError(f\"Attempted to get {n}th frame when only {n_frames} frames exist\")\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "    if not ret: \n",
    "        raise RuntimeError(f\"Failed to read {n}th frame\")\n",
    "    \n",
    "    return frame \n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR) :\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "else: \n",
    "    raise KeyboardInterrupt(\"Un-annotated dataset already exists, skipping\")\n",
    "\n",
    "for i in trange(TOTAL_SAMPLES):\n",
    "    n = np.random.randint(0, n_frames)\n",
    "    frame = get_nth_frame(cap, n, n_frames=n_frames)\n",
    "    cv2.imwrite(f\"./frames/frame-{n}.jpg\", frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation \n",
    "\n",
    "Please visit [roboflow](https://universe.roboflow.com/mice-maze/mice-maze) to view and download the annotated dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best backend to train the model on \n",
    "# If no such backend is available, such as a CUDA-enabled \n",
    "# GPU, found in modern Nvidia cards, or MPS found in \n",
    "# Apple devices with an Apple Silicon CPU (M1, M2, etc), \n",
    "# make use of the CPU \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else: \n",
    "    print(\"No backend availale, defaulting to CPU\")\n",
    "    print(\"This might take a while, please ensure you installed the correct version of pytorch for your hardware\")\n",
    "    print(\"Please see `https://pytorch.org/get-started/locally/`\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "results = yolo.train(data='mice-maze.yolov8/data.yaml', device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".yolo-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
