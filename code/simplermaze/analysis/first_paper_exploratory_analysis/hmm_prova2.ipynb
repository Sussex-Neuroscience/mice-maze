{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329b7a8c",
   "metadata": {},
   "source": [
    "This section provides a concrete, narrative guide to implementing this analysis using the Python ecosystem. The code logic described here integrates Traja for kinematics, dlc2kinematics for velocity processing, and hmmlearn for segmentation.\n",
    "\n",
    "### Step 1: Data Ingestion and Preprocessing\n",
    "The analysis begins with the raw $(x,y)$ data, from DLC (BUT NOW WE ARE GETTING IT FROM SLEAP).\n",
    "- Library: Traja.14\n",
    "- Smoothing: Raw tracking data contains high-frequency jitter that can inflate velocity estimates. We apply a Savitzky-Golay filter.\n",
    "- Code Concept: `trj.traja.smooth_sg(w=11, p=3)`. This fits a 3rd-order polynomial over an 11-frame window, preserving the sharp turns of VTE while removing noise.\n",
    "- Rediscretization: Mice move at variable speeds. To analyze the shape of the path independent of time, we can rediscretize the trajectory into steps of constant length (e.g., 1 cm).\n",
    "- Code Concept: `trj.traja.rediscretize(R=1.0)`. This is crucial for comparing the tortuosity of slow vs. fast trials.\n",
    "### Step 2: Feature Extraction Logic\n",
    "We calculate the feature matrix for the HMM.\n",
    "1. Velocity and Acceleration (dlc2kinematics)\n",
    "While Traja calculates speed, dlc2kinematics is optimized for DLC output and handles confidence scores well.22\n",
    "- Logic: Compute velocity vectors for the snout and centroid. Use the centroid for locomotion state and the snout for head-scanning metrics.\n",
    "2. Calculating IdPhi (Traja)\n",
    "- Logic: Calculate the turn angle between consecutive triplets of points.\n",
    "- angles = trj.traja.calc_turn_angle()\n",
    "- idphi = angles.abs().rolling(window=30).sum()\n",
    "- Nuance: A 1-second rolling window (approx 30 frames) is standard for capturing the \"bout\" of VTE.\n",
    "### Generating the Nesting Metric (Morphology)\n",
    "- Logic: If using standard DLC, calculate the distance between the \"Nose\" and \"TailBase\" points.\n",
    "- body_length = distance(Nose, TailBase)\n",
    "- During locomotion, body_length is maximal. During nesting/grooming, body_length decreases as the animal curls. Use the variance of this length over time as a feature.\n",
    "### Step 3: HMM Segmentation (hmmlearn)\n",
    "We utilize hmmlearn to perform the clustering.23\n",
    "- Data Preparation: Stack the features (Log-Speed, IdPhi, Body-Length-Variance) into a matrix $X$.\n",
    "- Normalization: HMMs (specifically Gaussian ones) are sensitive to scale. We must Z-score the data (mean=0, variance=1) using sklearn.preprocessing.StandardScaler.\n",
    "#### Model Definition:\n",
    "- model = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n",
    "- Initialization Strategy: To ensure the model finds the \"correct\" 3 states, it is often helpful to initialize the means using K-Means clustering first.\n",
    "#### Fitting and Prediction:\n",
    "model.fit(X) learns the transition and emission matrices.\n",
    "states = model.predict(X) assigns a label (0-2) to every frame.\n",
    "### Step 4: Post-Hoc Validation and Ethogram Construction\n",
    "The output of the HMM is a sequence of integers. We must map these to our ethological definitions.\n",
    "- Spatial Validation: Plot the occupancy heatmap for each state.\n",
    "- Nest State: Should cluster heavily in the Start Box or Corners.\n",
    "- Exploit State: Should form a \"highway\" between the start and the correct reward port.\n",
    "- Explore State: Should cover the maze edges (thigmotaxis) and the area immediately in front of the gratings (decision zone).\n",
    "- Temporal Validation: Plot the state probability over the session duration.\n",
    "We expect $P(S_{Nest})$ to increase linearly with time as satiety sets in. If $S_{Nest}$ is random, the model may be misidentifying \"freezing\" as nesting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Data...\n",
      "2. Cleaning Data (Threshold p < 0.5)...\n",
      "   Data cleaned and ready.\n",
      "3. Extracting Features...\n",
      "4. Training Model on Valid Trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in log\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Cluster Means:\n",
      "   log_velocity     idphi  body_length\n",
      "0     -0.678785 -0.826449    -0.260792\n",
      "1      0.972168  0.740301     0.494217\n",
      "2      0.076137  1.626100     0.223462\n",
      "3      0.814647 -0.223506    -0.081838\n",
      "Mapped States: {3: 'Explore (1)', 1: 'Exploit (2)', 0: 'Nest (3)', 2: 'Groom (4)'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9298c240d7e46b090fbd2cc4b5348fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='trial_id', options=('trial_000', 'trial_001', 'trial_002', 'trial_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import savgol_filter, medfilt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DLC_FILE = \"C:/Users/aleja/Box/Awake Project/Maze data/simplermaze/mouse 6357/deeplabcut/mouse6357/mouse6357-shahd-2025-09-08/videos/6357_2024-08-28_11_58_14s3.6DLC_Resnet50_mouse6357Sep8shuffle1_snapshot_200.csv\"\n",
    "\n",
    "TRIAL_FILE = \"C:/Users/aleja/Box/Awake Project/Maze data/simplermaze/mouse 6357/2024-08-28_11_58_146357session3.6/trials_corrected_final_frames.csv\"\n",
    "\n",
    "# Cleaning Parameters\n",
    "LIKELIHOOD_THRESH = 0.5  # Drop points with confidence below 10%\n",
    "MEDIAN_WINDOW = 5        # Remove single-frame jumps\n",
    "SMOOTH_WINDOW = 31       # Stronger smoothing for velocity (approx 1 sec)\n",
    "FPS = 30\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING & CLEANING\n",
    "# ==========================================\n",
    "def load_and_clean_data():\n",
    "    print(\"1. Loading Data...\")\n",
    "    df_dlc = pd.read_csv(DLC_FILE, header=[0, 1, 2], index_col=0)\n",
    "    scorer = df_dlc.columns.get_level_values(0)[0]\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Process each bodypart\n",
    "    print(f\"2. Cleaning Data (Threshold p < {LIKELIHOOD_THRESH})...\")\n",
    "    for bp in ['nose', 'mid', 'tailbase']:\n",
    "        # Extract raw columns\n",
    "        x = df_dlc[scorer][bp]['x']\n",
    "        y = df_dlc[scorer][bp]['y']\n",
    "        p = df_dlc[scorer][bp]['likelihood']\n",
    "        \n",
    "        # FILTER: Set low confidence points to NaN\n",
    "        bad_points = p < LIKELIHOOD_THRESH\n",
    "        x[bad_points] = np.nan\n",
    "        y[bad_points] = np.nan\n",
    "        \n",
    "        # INTERPOLATE: Fill gaps linearly\n",
    "        x = x.interpolate(method='linear', limit_direction='both')\n",
    "        y = y.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        # SMOOTH: Median filter removes sharp \"teleporting\" jumps\n",
    "        x = medfilt(x, kernel_size=MEDIAN_WINDOW)\n",
    "        y = medfilt(y, kernel_size=MEDIAN_WINDOW)\n",
    "        \n",
    "        data[f'{bp}_x'] = x\n",
    "        data[f'{bp}_y'] = y\n",
    "        data[f'{bp}_p'] = p  # Keep raw p for reference\n",
    "\n",
    "    # Load Trials\n",
    "    df_trials = pd.read_csv(TRIAL_FILE)\n",
    "    \n",
    "    return data, df_trials\n",
    "\n",
    "df_tracking, df_trials = load_and_clean_data()\n",
    "print(\"   Data cleaned and ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. FEATURE ENGINEERING (ROBUST)\n",
    "# ==========================================\n",
    "print(\"3. Extracting Features...\")\n",
    "\n",
    "# A. Velocity (Smoothed)\n",
    "dx = np.diff(df_tracking['mid_x'], prepend=df_tracking['mid_x'][0])\n",
    "dy = np.diff(df_tracking['mid_y'], prepend=df_tracking['mid_y'][0])\n",
    "raw_vel = np.sqrt(dx**2 + dy**2) * FPS\n",
    "# Heavy smoothing to fix \"jitter\" noise\n",
    "df_tracking['velocity'] = savgol_filter(raw_vel, window_length=SMOOTH_WINDOW, polyorder=3)\n",
    "df_tracking['log_velocity'] = np.log(df_tracking['velocity'] + 1e-6)\n",
    "\n",
    "# B. IdPhi (Heading Changes)\n",
    "# Note: Nose tracking is poor (12% good), so this feature might still be noisy.\n",
    "nose_dx = np.diff(df_tracking['nose_x'], prepend=df_tracking['nose_x'][0])\n",
    "nose_dy = np.diff(df_tracking['nose_y'], prepend=df_tracking['nose_y'][0])\n",
    "heading = np.degrees(np.arctan2(nose_dy, nose_dx))\n",
    "dphi = np.diff(heading, prepend=heading[0])\n",
    "dphi = (dphi + 180) % 360 - 180 \n",
    "# Stronger rolling sum to smooth out heading jitter\n",
    "df_tracking['idphi'] = pd.Series(np.abs(dphi)).rolling(window=SMOOTH_WINDOW, center=True).sum().fillna(0)\n",
    "\n",
    "# C. Body Length\n",
    "df_tracking['body_length'] = np.sqrt(\n",
    "    (df_tracking['nose_x'] - df_tracking['tailbase_x'])**2 + \n",
    "    (df_tracking['nose_y'] - df_tracking['tailbase_y'])**2\n",
    ")\n",
    "# Smooth body length too\n",
    "df_tracking['body_length'] = savgol_filter(df_tracking['body_length'], window_length=SMOOTH_WINDOW, polyorder=3)\n",
    "\n",
    "# ==========================================\n",
    "# 4. HMM SEGMENTATION\n",
    "# ==========================================\n",
    "print(\"4. Training Model on Valid Trials...\")\n",
    "\n",
    "# Tag Valid Frames\n",
    "valid_mask = np.zeros(len(df_tracking), dtype=bool)\n",
    "for _, row in df_trials.iterrows():\n",
    "    start, end = int(row['start_frame']), int(row['end_frame'])\n",
    "    if start < len(df_tracking) and end < len(df_tracking):\n",
    "        valid_mask[start:end] = True\n",
    "\n",
    "# Prepare Data\n",
    "features = ['log_velocity', 'idphi', 'body_length']\n",
    "X = df_tracking[features].values\n",
    "X_train = df_tracking.loc[valid_mask, features].values\n",
    "X_train = np.nan_to_num(X_train)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit HMM\n",
    "model = hmm.GaussianHMM(n_components=4, covariance_type=\"full\", n_iter=100, random_state=42)\n",
    "model.fit(X_train_scaled)\n",
    "\n",
    "# Predict\n",
    "X_all_scaled = scaler.transform(np.nan_to_num(X))\n",
    "df_tracking['state_hmm'] = model.predict(X_all_scaled)\n",
    "\n",
    "# ==========================================\n",
    "# 5. AUTOMATIC STATE LABELING\n",
    "# ==========================================\n",
    "# Map states 0-3 to Explore(1), Exploit(2), Nest(3), Groom(4)\n",
    "means = pd.DataFrame(model.means_, columns=features)\n",
    "print(\"\\nState Cluster Means:\")\n",
    "print(means)\n",
    "\n",
    "# 1. Exploit = Highest Velocity\n",
    "exploit_cluster = means['log_velocity'].idxmax()\n",
    "# 2. Nest = Lowest Velocity\n",
    "nest_cluster = means['log_velocity'].idxmin()\n",
    "# 3. Explore vs Groom\n",
    "remaining = list(set(range(4)) - {exploit_cluster, nest_cluster})\n",
    "# Explore has higher velocity than Groom\n",
    "if means.loc[remaining[0], 'log_velocity'] > means.loc[remaining[1], 'log_velocity']:\n",
    "    explore_cluster, groom_cluster = remaining[0], remaining[1]\n",
    "else:\n",
    "    explore_cluster, groom_cluster = remaining[1], remaining[0]\n",
    "\n",
    "state_map = {explore_cluster: 'Explore (1)', exploit_cluster: 'Exploit (2)', \n",
    "             nest_cluster: 'Nest (3)', groom_cluster: 'Groom (4)'}\n",
    "state_id_map = {explore_cluster: 1, exploit_cluster: 2, nest_cluster: 3, groom_cluster: 4}\n",
    "\n",
    "df_tracking['state_name'] = df_tracking['state_hmm'].map(state_map)\n",
    "df_tracking['state_id'] = df_tracking['state_hmm'].map(state_id_map)\n",
    "print(f\"Mapped States: {state_map}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. INTERACTIVE PLOT\n",
    "# ==========================================\n",
    "def plot_trial(trial_id):\n",
    "    row = df_trials[df_trials['matched_trial_id'] == trial_id].iloc[0]\n",
    "    start, end = int(row['start_frame']), int(row['end_frame'])\n",
    "    \n",
    "    # Get Data Slice\n",
    "    df_slice = df_tracking.loc[start:end].copy()\n",
    "    \n",
    "    # Calculate Sequences\n",
    "    pred_seq = \",\".join(df_slice['state_id'].loc[df_slice['state_id'].shift() != df_slice['state_id']].astype(str))\n",
    "    print(f\"Trial: {trial_id}\")\n",
    "    print(f\"Manual Sequence: {row.get('state_sequence', 'N/A')}\")\n",
    "    print(f\"Predicted Seq:   {pred_seq}\")\n",
    "\n",
    "    # Plot\n",
    "    colors = {'Explore (1)': '#ff7f0e', 'Exploit (2)': '#2ca02c', 'Nest (3)': '#1f77b4', 'Groom (4)': '#9467bd'}\n",
    "    \n",
    "    fig = px.scatter(df_slice, x='mid_x', y='mid_y', color='state_name', \n",
    "                     color_discrete_map=colors, title=f\"Cleaned Trajectory: {trial_id}\",\n",
    "                     labels={'mid_x':'X', 'mid_y':'Y'}, height=500, width=600)\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout(yaxis_scaleanchor=\"x\")\n",
    "    fig.update_traces(marker=dict(size=4)) # Smaller markers for cleaner look\n",
    "    fig.show()\n",
    "\n",
    "# Dropdown\n",
    "trial_list = df_trials['matched_trial_id'].unique().tolist()\n",
    "widgets.interact(plot_trial, trial_id=widgets.Dropdown(options=trial_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac361dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hmmlearn in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (from hmmlearn) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (from hmmlearn) (1.6.1)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (from hmmlearn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aleja\\anaconda3\\envs\\malawi\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hmmlearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malawi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
